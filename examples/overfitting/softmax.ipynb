{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting, overfitting, regularization, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and minibatch MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mLoading MNIST...\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(600, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "Atype = gpu() >= 0 ? KnetArray{Float32} : Array{Float32}\n",
    "dtst = minibatch(xtst,ytst,100;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "dtrn = minibatch(xtrn,ytrn,100;xtype=Atype) # [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100\n",
    "length(dtrn),length(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28×28×1×100 Knet.KnetArray{Float32,4}\n",
      "100-element Array{UInt8,1}\n"
     ]
    }
   ],
   "source": [
    "(x,y)=first(dtst)\n",
    "println(summary(x))\n",
    "println(summary(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAAYagMeiWXwAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsSAAALEgHS3X78AAABWUlEQVRo3u3XvSuFYRjH8VNeMqijKGZWnQkpZSKrTOy2k5DRHyAsysAgeUlZDAYKf4CBhVHIKBYMXoYH32u46yTUqfs+p6t+n/np+fbU9dwvuZyIuJfHNuagoL9gGw5wgzf0Q0E/wU7sIisxCgX9BLvRh1ko6DMYLMJiHxiEgv6C+7DgEpLHFIxuDLbxWrAFCvoKDuATFttDLRT0FbQf/QvnaETSmIJR2Q8+iRPY0Cwg6ZcpGF0PXrGOW7RDQT/BBlziAXcYQdKvUzA6W6TD5XMcSWMKRreKY1jsDE1Q0EewDsN4RBiYInpL2AJuz0W50CgYNdiBU2R/CJeZe9iB+AobmIddVluhYPWCM3hG9o8Q/Okd06iHgtUL7sAOvL+98AkX2MIm7NkJHMEGZgVlTaeCSYJTeEGIXOMQayig7BcqWPFgF2zTXcYQmlGD6CEFkwVFRERERCroG8MZvDxV6KNpAAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " ⋮                                                           ⋱                                        \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAAYagMeiWXwAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsSAAALEgHS3X78AAABX0lEQVRo3u3ZK0sEYRiG4UUEEcGmRcFi0mbZIPgDzIJdsKpg9BRcDGLxgMVfoMlkFIPFrFgNBhFM4gFhF70f2BeGRYt+I/PBc/2AuSfMvvN+s7WamZn9qzouMIs+OJhf8BhNtHCCcTiYV3ARH1BQ4UfMoxsO5hGUZdyjWbCNITiYR1A0vJewj4gewsF8gqEXDbzjE2twMK9gOIUG+iV64GB+QT088VIegIP5BefgYJ7BGN5vUPAMpS7FDv7aHnRBLcAH2MQKrtBqu0GyZdjB5MEjFA8v8SOXB6wi6WHGweRBLUbTWMctnhDBXSQLOVhasFM/dhAPzTAczCsoE3iBomNwsJpBDeUp/BR8hQa5g9UMzuAZnS/WUWipukYMcAerF+yCPvboYoNtI9jAHWKJ0g1t4c8HGAeTBxX4bukN51hAsgnjYPKgHgJ9ICgGdaDRHyOTSBJxsNSgmZmZWWa+ALsaWXzkTiBTAAAAAElFTkSuQmCC",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " ⋮                                                           ⋱                                        \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAAYagMeiWXwAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsSAAALEgHS3X78AAAB4UlEQVRo3u3aPyjnYQDHcQu5DG663KAkFrYTCUXhSmc2WS4GpSxISRYMmAzsimK58mdww7mupMMiXZ0UBnI3yYAi4v0ZfvVN/ArfH0/6vBbD83je1PP7Pr/fQ1qamZmZPdo75GAUf3CNbjQiCw6GHxzHARS6iviHDVTCwTCD+VjCAP5CwU20oh6F0FgtHAwvWIJeaHMMYwtnqEAsv42DKQ82QbFdzELRfmisFA6GH1zAL+xhEg1IjHVhGjqAHQw3WIQWtOErshEdH4I21Qc4GF5wBoM4RA90+L5HdE4dzrGNZx/ADsYePELiTa82Ry6SzfsOB8MI1kCLjEEL6Y1TBh6arwNZ8/RBJw8Ovn7wM7TAIk6xgmTz9QbrBmvQhx4HXz/4EfvQpc8ImvHQ3CroQkE/oL7nSQ9wB2MPygR0sGoTJFukDz+g4Dc8KeZg7MFi6Kse4DvQxdB983QxpAdDOxRMtrkcfNmgDlTFLvEfmbhvnjZU4pLWwbCCsgotok3xCXfHO3AdoQPYwbCCerEfQ4tNITqWjnlE/1hygS9wMJygVEOXsidYiviJqzs68ayYgykJSgHmEF08cdGgS4R1lCOWmIMpCYpe6GXQhftvLEMXD7poSMk/DDgYe9DMzOwNuwXWmzJaTPaYHgAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " ⋮                                                           ⋱                                        \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)  Gray{Float32}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{UInt8,1}:\n",
       " 0x02\n",
       " 0x03\n",
       " 0x0a"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rp = randperm(10000)\n",
    "for i=1:3; display(mnistview(xtst,rp[i])); end\n",
    "display(ytst[rp[1:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winit1 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear(w,x)\n",
    "    y = w[1]*mat(x) .+ w[2]\n",
    "end\n",
    "\n",
    "winit1()=map(Atype, [ 0.1*randn(10,784), zeros(10,1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(9)\n",
    "w = winit1();  # random weight matrix and a zero bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Knet.KnetArray{Float32,2}:\n",
       "  0.20949      0.204775   -0.932485   -0.97682     0.694272  …  -0.332532  -0.510457   -1.40151    -0.76995    -0.152675\n",
       "  0.455851    -0.707735   -0.22817    -0.0821219   1.12012       0.259282  -0.132053   -0.523914    0.0569146   0.522016\n",
       " -0.00504193  -2.14745     0.0253635  -1.09536    -1.06499       1.02387   -0.0384934  -0.288532   -0.825279   -1.07451 \n",
       "  1.32813     -1.03565     0.237599    0.136671    0.455203      0.508845  -0.0922899   0.607264    0.487591   -0.231088\n",
       "  0.351397    -0.638922   -1.09847    -0.554185   -0.654054     -0.296868  -0.580158    0.316118   -0.460735   -1.00915 \n",
       " -0.180563    -0.725433   -0.8814     -0.572751   -0.506427  …  -1.66223    0.277045   -1.90686     0.0757486  -1.3144  \n",
       "  0.370442     0.0319697  -0.865752   -0.644942    0.797276     -0.762742  -0.698524   -1.2362      0.79252     0.637139\n",
       " -0.105419     0.478139    0.305943    0.828578    0.245938      0.900937   0.494892    0.516854    1.07525     1.38668 \n",
       " -0.714492    -1.24502    -0.0765007  -0.646126    0.364152      1.02073    0.343257   -0.0855907  -0.0235416   0.26818 \n",
       "  1.73877      2.96371     0.476608    3.17109     1.55325       3.84604    0.38543     2.12461     1.48999     2.68136 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ypred=linear(w,x)\n",
    "display(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{UInt8,1}:\n",
       " 0x07\n",
       " 0x02\n",
       " 0x01\n",
       " 0x0a\n",
       " 0x04\n",
       " 0x01\n",
       " 0x04\n",
       " 0x09\n",
       " 0x05\n",
       " 0x09\n",
       " 0x0a\n",
       " 0x06\n",
       " 0x09\n",
       "    ⋮\n",
       " 0x06\n",
       " 0x01\n",
       " 0x03\n",
       " 0x06\n",
       " 0x09\n",
       " 0x03\n",
       " 0x01\n",
       " 0x04\n",
       " 0x01\n",
       " 0x07\n",
       " 0x06\n",
       " 0x09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0971"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(w,dtst,linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softloss (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cross entropy loss of a model with weights w for one minibatch (x,p)\n",
    "# Use non-zero l1 or l2 for regularization\n",
    "function softloss(w,x,p,model;l1=0,l2=0,o...)  \n",
    "    y = model(w,x;o...)\n",
    "    J = nll(y,p)\n",
    "    if l1 != 0; J += l1 * sum(sum(abs,wi)  for wi in w[1:2:end]); end\n",
    "    if l2 != 0; J += l2 * sum(sum(abs2,wi) for wi in w[1:2:end]); end\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8222036f0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(linear(w,x),y)  # per-instance average softloss for the first test minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8204281f0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(w,dtst,linear)  # per-instance average softloss for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually defined gradient for softloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= TODO\n",
    "function grad1(w,x,p,model)\n",
    "    y = model(w,x)\n",
    "    y = y .- maximum(y,1) # for numerical stability\n",
    "    expy = exp.(y)\n",
    "    q = expy ./ sum(expy,1)\n",
    "    dJdy = (q - p) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = sum(dJdy,2)\n",
    "    Any[dJdw,dJdb]\n",
    "end\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically defined gradient for softloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softgrad = grad(softloss)  # Knet/AutoGrad makes life easier :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO g1 = grad1(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081051ff400, 31360, 0, nothing), (10, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x000000810530ea00, 40, 0, nothing), (10, 1))     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = softgrad(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO isapprox(g1[1],g2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO isapprox(g1[2],g2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1 Knet.KnetArray{Float32,2}:\n",
       " -0.0921663 \n",
       " -0.00683322\n",
       " -0.0612594 \n",
       " -0.0592853 \n",
       " -0.0203152 \n",
       " -0.0529135 \n",
       " -0.0752202 \n",
       "  0.0736465 \n",
       " -0.0507283 \n",
       "  0.345075  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(g2[2])  \n",
    "# Meaning of gradient:\n",
    "# If I move the last entry of w[2] by epsilon, loss will go up by 0.345075 epsilon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1 Knet.KnetArray{Float32,2}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8222036f0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softloss(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[2][10] += 0.1   # to numerically check the gradient let's move the last entry by +0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8577392f0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softloss(w,x,y,linear)  \n",
    "# We see that the loss moves by +0.03 as expected.\n",
    "# You should check all/most entries in your gradients this way to make sure they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,model; epochs=100,lr=0.5,o...)\n",
    "    weights = Any[copy(w)]\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = softgrad(w,x,y,model;o...)\n",
    "            for i in 1:length(w)\n",
    "                w[i] = w[i] - lr * g[i]\n",
    "            end\n",
    "        end\n",
    "        push!(weights,copy(w))\n",
    "    end\n",
    "    return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25.327196 seconds (24.66 M allocations: 18.528 GiB, 4.35% gc time)\n",
      " 17.146108 seconds (8.32 M allocations: 18.025 GiB, 5.21% gc time)\n",
      "  2.864833 seconds (1.39 M allocations: 3.005 GiB, 5.29% gc time)\n",
      " 18.549852 seconds (5.77 M allocations: 18.568 GiB, 4.68% gc time)\n",
      "  3.243446 seconds (967.86 k allocations: 3.095 GiB, 4.51% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29008842f0, 0.08150000000000002)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(1)\n",
    "@time trn1=train(winit1(),dtrn,linear)\n",
    "@time trnloss1 = [ nll(w,dtrn,linear) for w in trn1 ]\n",
    "@time tstloss1 = [ nll(w,dtst,linear) for w in trn1 ]\n",
    "@time trnerr1 = [ 1-accuracy(w,dtrn,linear) for w in trn1 ]\n",
    "@time tsterr1 = [ 1-accuracy(w,dtst,linear) for w in trn1 ]\n",
    "minimum(tstloss1),minimum(tsterr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(trn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1],ylim=(.2,.36),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates both overfitting and underfitting\n",
    "# Overfitting: test loss is higher than training loss and getting worse\n",
    "# Underfitting: training loss not close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1],ylim=(.06,.10),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 8% error, i.e. 92% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try L2 regularization to address overfitting\n",
    "srand(1)\n",
    "@time trn2=train(winit1(),dtrn,linear;l2=0.00004)\n",
    "@time trnloss2 = [avgloss(w,dtrn,linear) for w in trn2]\n",
    "@time tstloss2 = [avgloss(w,dtst,linear) for w in trn2]\n",
    "@time trnerr2 = [zeroone(w,dtrn,linear) for w in trn2]\n",
    "@time tsterr2 = [zeroone(w,dtst,linear) for w in trn2]\n",
    "minimum(tstloss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1 trnloss2 tstloss2],ylim=(0.2,0.36),\n",
    "    labels=[:trnloss :tstloss :trnlossL2 :tstlossL2],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# overfitting less but results do not improve much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1 trnerr2 tsterr2],ylim=(0.06,0.10),\n",
    "    labels=[:trnerr :tsterr :trnerrL2 :tsterrL2],xlabel=\"Epochs\",ylabel=\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a model with higher capacity helps underfitting\n",
    "function mlp(w,x)\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function winit(h...; std=0.01, x=784, y=10,  # use winit(h1,h2,...,hn) for n hidden layer mlp\n",
    "               atype=gpu()>=0 ? KnetArray{Float32} : Array{Float32})\n",
    "    h = [x, h..., y]\n",
    "    w = Any[]\n",
    "    for i=1:length(h)-1\n",
    "        push!(w, std*randn(h[i+1],h[i]))\n",
    "        push!(w, zeros(h[i+1],1))\n",
    "    end\n",
    "    map(atype, w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=winit(64) # gives weights and biases for an MLP with a single hidden layer of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softloss(w2,x,y,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn3=train(winit(64),dtrn,mlp)\n",
    "@time trnloss3 = [ avgloss(w,dtrn,mlp) for w in trn3 ]\n",
    "@time tstloss3 = [ avgloss(w,dtst,mlp) for w in trn3 ]\n",
    "@time trnerr3 = [ zeroone(w,dtrn,mlp) for w in trn3 ]\n",
    "@time tsterr3 = [ zeroone(w,dtst,mlp) for w in trn3 ]\n",
    "minimum(tstloss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss1 tstloss1 trnloss3 tstloss3],ylim=(0.0,0.4),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Loss\")  \n",
    "# solves the underfitting problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr1 tsterr1 trnerr3 tsterr3],ylim=(0,0.1),\n",
    "    labels=[:trnLin :tstLin :trnMLP :tstMLP],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# error improves from 8% to 2%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have overfitting, let's try L1 regularization and a lower learning rate\n",
    "srand(1)\n",
    "@time trn4=train(winit(64),dtrn,mlp;lr=0.1,l1=4e-5)\n",
    "@time trnloss4 = [ avgloss(w,dtrn,mlp) for w in trn4 ]\n",
    "@time tstloss4 = [ avgloss(w,dtst,mlp) for w in trn4 ]\n",
    "@time trnerr4 = [ zeroone(w,dtrn,mlp) for w in trn4 ]\n",
    "@time tsterr4 = [ zeroone(w,dtst,mlp) for w in trn4 ]\n",
    "minimum(tstloss4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss3 tstloss3 trnloss4 tstloss4],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnMLP_L1 :tstMLP_L1],xlabel=\"Epochs\", ylabel=\"Loss\")  \n",
    "# overfitting less, loss results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr3 tsterr3 trnerr4 tsterr4],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnMLP_L1 :tstMLP_L1],xlabel=\"Epochs\", ylabel=\"Error\")    \n",
    "# however error results do not improve! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tsterr3),minimum(tsterr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout is another way to address overfitting\n",
    "function dropout(x,p)\n",
    "    if p > 0\n",
    "        x .* (rand!(similar(x)) .> p) ./ (1-p)\n",
    "    else\n",
    "        x\n",
    "    end\n",
    "end\n",
    "\n",
    "function mlp(w,x; pdrop=(0,0))\n",
    "    x = dropout(x,pdrop[1])\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "        x = dropout(x,pdrop[2])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(1)\n",
    "@time trn5=train(winit(64),dtrn,mlp;lr=0.1,pdrop=(0.2,0))\n",
    "@time trnloss5 = [ avgloss(w,dtrn,mlp) for w in trn5 ]\n",
    "@time tstloss5 = [ avgloss(w,dtst,mlp) for w in trn5 ]\n",
    "@time trnerr5 = [ zeroone(w,dtrn,mlp) for w in trn5 ]\n",
    "@time tsterr5 = [ zeroone(w,dtst,mlp) for w in trn5 ]\n",
    "minimum(tstloss5),minimum(tsterr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss3 tstloss3 trnloss5 tstloss5],ylim=(0,0.15),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Loss\")  \n",
    "# overfitting less, loss results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr3 tsterr3 trnerr5 tsterr5],ylim=(0,0.04),\n",
    "    labels=[:trnMLP :tstMLP :trnDropout :tstDropout],xlabel=\"Epochs\", ylabel=\"Error\")  \n",
    "# this time error also improves slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tsterr3),minimum(tsterr4),minimum(tsterr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(tstloss3),minimum(tstloss4),minimum(tstloss5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with larger hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current trend is to use models with higher capacity tempered with dropout\n",
    "srand(1)\n",
    "@time trn6=train(winit(256),dtrn,mlp;lr=0.1,pdrop=(0.2,0))\n",
    "@time trnloss6 = [ avgloss(w,dtrn,mlp) for w in trn6 ]\n",
    "@time tstloss6 = [ avgloss(w,dtst,mlp) for w in trn6 ]\n",
    "@time trnerr6 = [ zeroone(w,dtrn,mlp) for w in trn6 ]\n",
    "@time tsterr6 = [ zeroone(w,dtst,mlp) for w in trn6 ]\n",
    "minimum(tstloss6),minimum(tsterr6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss5 tstloss5 trnloss6 tstloss6],ylim=(0,0.15),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr5 tsterr5 trnerr6 tsterr6],ylim=(0,0.04),\n",
    "    labels=[:trn64 :tst64 :trn256 :tst256],xlabel=\"Epochs\",ylabel=\"Error\")\n",
    "# We are down to 0.015 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
