{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Plots, JLD, NBInclude\n",
    "nbinclude(\"mnist.ipynb\")  # loads MNIST, defines dtrn,dtst,Atype,train,softmax,zeroone\n",
    "ENV[\"COLUMNS\"]=80         # column width for array printing\n",
    "plotlyjs();               # for interactive plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict function returns a score for each class as a linear function of input x\n",
    "function linear(w,x)\n",
    "    y = w[1]*mat(x) .+ w[2]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights as a list of [ weightMatrix, biasVector ]\n",
    "winit1(;std=0.01)=map(Atype, [ std*randn(10,784), zeros(10,1) ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and zero-one loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"28×28×1×100 Knet.KnetArray{Float32,4}\", \"100-element Array{UInt8,1}\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab a minibatch\n",
    "x,y = first(dtst)\n",
    "map(summary,(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Array{Float32,2}:\n",
       "  0.020949      0.0204775   -0.0932485   …  -0.0769949   -0.0152675\n",
       "  0.0455851    -0.0707735   -0.0228169       0.00569148   0.0522016\n",
       " -0.000504205  -0.214745     0.00253635     -0.082528    -0.107451 \n",
       "  0.132813     -0.103564     0.0237599       0.0487591   -0.0231088\n",
       "  0.0351397    -0.0638922   -0.109847       -0.0460735   -0.100915 \n",
       " -0.0180563    -0.0725433   -0.08814     …   0.00757484  -0.13144  \n",
       "  0.0370442     0.00319698  -0.0865752       0.079252     0.0637138\n",
       " -0.010542      0.0478139    0.0305943       0.107525     0.138668 \n",
       " -0.0714492    -0.124502    -0.00765008     -0.00235417   0.026818 \n",
       "  0.173877      0.296371     0.0476608       0.148999     0.268136 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random model and calculate predictions for one minibatch\n",
    "setseed(9)           # for replicability\n",
    "w = winit1()         # random weight matrix and a zero bias vector\n",
    "ypred = linear(w,x)  # predict\n",
    "Array(ypred)         # predictions are given as a 10xN score matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×100 RowVector{UInt8,Array{UInt8,1}}:\n",
       " 0x07  0x02  0x01  0x0a  0x04  0x01  …  0x01  0x04  0x01  0x07  0x06  0x09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y'  # correct answers are given as an array of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy gives percentage of correct answers\n",
    "accuracy(ypred,y)        # 2-arg version: accuracy on this batch of 100 with initial w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0971"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(w,dtst,linear)  # 3-arg version: accuracy on the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9029"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeroone loss (error) defined as 1 - accuracy\n",
    "zeroone(w,data,model) = 1 - accuracy(w,data,model)\n",
    "zeroone(w,dtst,linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate softmax (cross entropy, negative log likelihood) loss of a model with weights w for one minibatch (x,y)\n",
    "# Predict specifies the function for model output: ypred = predict(w,x;o...)\n",
    "softmax(w,x,y,predict; o...)=nll(predict(w,x;o...),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.30209f0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-instance average softloss for the first test minibatch, should be close to -log(1/10)=2.3\n",
    "softmax(w,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.30209f0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual loss calculation\n",
    "ypred=linear(w,x)\n",
    "yp1 = exp.(ypred)\n",
    "yp2 = yp1 ./ sum(yp1,1)\n",
    "yp3 = -log.(yp2)\n",
    "yc1 = full(sparse(y,1:100,1f0))\n",
    "sum(Array(yp3).*yc1) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2936275f0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(w,data,predict) = mean(softmax(w,x,y,predict) for (x,y) in data)\n",
    "softmax(w,dtst,linear)  # per-instance average softmax loss for the whole test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the gradient using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically defined gradient for softloss\n",
    "softgrad = grad(softmax)  # softgrad returns gradient wrt 1st arg w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Knet.KnetArray{Float32,2},1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081052f7c00, 31360, 0, nothing), (10, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053f0e00, 40, 0, nothing), (10, 1))     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setseed(9)\n",
    "w1 = winit1(std=0.1)  # use a larger std to get a larger gradient for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081052ff800, 31360, 0, nothing), (10, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053f1000, 40, 0, nothing), (10, 1))     "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = softgrad(w1,x,y,linear)  # gradient has the same size and shape as the first parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the gradient using numerical approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float32,2}:\n",
       " -0.0921663  -0.00683322  -0.0612594  …  0.0736465  -0.0507283  0.345075"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(g1[2])'  \n",
    "# Meaning of gradient:\n",
    "# If I move the last entry of w[2] by epsilon, loss will go up by 0.345075 epsilon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float32,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(w1[2])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8222036f0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float32,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[2][10] = 0.1   # to numerically check the gradient let's move the last entry by +0.1.\n",
    "Array(w1[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8577392f0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(w1,x,y,linear)  \n",
    "# We see that the loss moves by +0.03 as expected.\n",
    "# You should check all/most entries in your gradients this way to make sure they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the gradient using manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defined gradient for softloss\n",
    "function softgrad_manual(w,x,y,predict)\n",
    "    x = mat(x)\n",
    "    p = predict(w,x)\n",
    "    p = p .- maximum(p,1) # for numerical stability\n",
    "    expp = exp.(p)\n",
    "    p = expp ./ sum(expp,1)\n",
    "    q = oftype(p, sparse(convert(Vector{Int},y),1:length(y),1,size(p,1),length(y)))\n",
    "    dJdy = (p - q) / size(x,2)\n",
    "    dJdw = dJdy * x'\n",
    "    dJdb = sum(dJdy,2)\n",
    "    Any[dJdw,dJdb]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x0000008105307400, 31360, 0, nothing), (10, 784))\n",
       " Knet.KnetArray{Float32,2}(Knet.KnetPtr(Ptr{Void} @0x00000081053f1200, 40, 0, nothing), (10, 1))     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = softgrad_manual(w1,x,y,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1[1] ≈ g2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1[2] ≈ g2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (SGD) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model(w) with SGD and return a list containing w for every epoch\n",
    "function train(w,data,predict; epochs=100,lr=0.1,o...)\n",
    "    weights = Any[deepcopy(w)]\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in data\n",
    "            g = softgrad(w,x,y,predict;o...)\n",
    "            update!(w,g,lr=lr)  # w[i] = w[i] - lr * g[i]\n",
    "        end\n",
    "        push!(weights,deepcopy(w))\n",
    "    end\n",
    "    return weights\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26674008f0, 0.07440000000000002)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isfile(\"lin.jld\")\n",
    "    setseed(1)\n",
    "    @time weights = train(winit1(),dtrn,linear,lr=0.1)           # 31.1s\n",
    "    @time trnloss = [ softmax(w,dtrn,linear) for w in weights ]  # 22.2s\n",
    "    @time tstloss = [ softmax(w,dtst,linear) for w in weights ]  # 3.7s\n",
    "    @time trnerr  = [ zeroone(w,dtrn,linear) for w in weights ]  # 20.6s\n",
    "    @time tsterr  = [ zeroone(w,dtst,linear) for w in weights ]  # 3.4s\n",
    "    @save \"lin.jld\" trnloss tstloss trnerr tsterr\n",
    "else\n",
    "    @load \"lin.jld\"\n",
    "end\n",
    "minimum(tstloss),minimum(tsterr)  # 0.2667, 0.0744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnloss tstloss],ylim=(.2,.36),labels=[:trnloss :tstloss],xlabel=\"Epochs\",ylabel=\"Loss\") \n",
    "# Demonstrates underfitting: training loss not close to 0\n",
    "# Also slight overfitting: test loss higher than train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([trnerr tsterr],ylim=(.06,.10),labels=[:trnerr :tsterr],xlabel=\"Epochs\",ylabel=\"Error\")  \n",
    "# this is the error plot, we get to about 7.5% test error, i.e. 92.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
