{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this is an instructional example written in low-level Julia/Knet and slow to train.\n",
    "# For a faster and high-level implementation please see `@doc rnninit` and `@doc rnnforw`.\n",
    "\n",
    "using Knet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A one layer MLP vs a simple RNN\n",
    "\n",
    "([Elman 1990](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/pdf)) A simple RNN takes the previous hidden state as an extra input, and returns the next hidden state as an extra output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" width=\"150\" />\n",
    "([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of a single hidden layer MLP and corresponding RNN\n",
    "\n",
    "function mlp1(param, input)\n",
    "    hidden = tanh(input * param[1] .+ param[2])\n",
    "    output = hidden * param[3] .+ param[4]\n",
    "    return output\n",
    "end\n",
    "\n",
    "function rnn1(param, input, hidden)\n",
    "    input2 = hcat(input, hidden)\n",
    "    hidden = tanh(input2 * param[1] .+ param[2])\n",
    "    output = hidden * param[3] .+ param[4]\n",
    "    return (hidden, output)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time (BPTT)\n",
    "\n",
    "([Werbos, 1988](http://www.sciencedirect.com/science/article/pii/089360808890007X))\n",
    "An RNN unrolled in time is similar to a deep feed-forward network which (i) has as many layers as time steps, (ii) has weights shared between different layers, and (iii) may have multiple inputs and outputs received and produced at individual layers. Backpropagation can be used to train RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=800 />\n",
    "([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss calculation and training.\n",
    "\n",
    "function rnnloss(param,inputs,hidden,outputs)\n",
    "    # inputs and outputs are sequences of the same length\n",
    "    sumloss = 0\n",
    "    for t in 1:length(inputs)\n",
    "        output,hidden = rnn1(param,inputs[t],hidden)\n",
    "        sumloss += loss_function(output,outputs[t])\n",
    "    end\n",
    "    return sumloss\n",
    "end\n",
    "\n",
    "rnngrad = grad(rnnloss);\n",
    "\n",
    "# ... train with our usual SGD procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "([Hochreiter and Schmidhuber, 1997](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))\n",
    "LSTM is a more sophisticated RNN module that performs better with long-range dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=800 />\n",
    "([image source](http://colah.github.io/posts/2015-08-Understanding-LSTMs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "f_t &= \\sigma(W_f\\cdot[h_{t-1},x_t] + b_f) & \\text{forget gate} \\\\\n",
    "i_t &= \\sigma(W_i\\cdot[h_{t-1},x_t] + b_i) & \\text{input gate} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C\\cdot[h_{t-1},x_t] + b_C) & \\text{cell candidate} \\\\\n",
    "C_t &= f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t & \\text{new cell} \\\\\n",
    "o_t &= \\sigma(W_o\\cdot[h_{t-1},x_t] + b_o) & \\text{output gate} \\\\\n",
    "h_t &= o_t \\ast \\tanh(C_t) & \\text{new output}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.google.com/drawings/d/1BR871g8k4jpI-mKeXiJfpY5Jl5cKcognvH7hHSugQds/pub?w=958&h=236\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM implementation in Knet\n",
    "\n",
    "function lstm(param, state, input)\n",
    "    weight,bias = param\n",
    "    hidden,cell = state\n",
    "    h       = size(hidden,2)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    forget  = sigm.(gates[:,1:h])\n",
    "    ingate  = sigm.(gates[:,1+h:2h])\n",
    "    outgate = sigm.(gates[:,1+2h:3h])\n",
    "    change  = tanh.(gates[:,1+3h:4h])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh.(cell)\n",
    "    return (hidden,cell)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence model (S2S)\n",
    "([Sutskever et al. 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf))\n",
    "S2S models learn to map input sequences to output sequences using an encoder and a decoder RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nzw0301.github.io/images/seq2seq.svg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2S model initialization\n",
    "\n",
    "function initmodel(H, V; atype=(gpu()>=0 ? KnetArray{Float32} : Array{Float32}))\n",
    "    init(d...)=atype(xavier(d...))\n",
    "    model = Dict{Symbol,Any}()\n",
    "    model[:state0] = [ init(1,H), init(1,H) ]\n",
    "    model[:embed1] = init(V,H)\n",
    "    model[:encode] = [ init(2H,4H), init(1,4H) ]\n",
    "    model[:embed2] = init(V,H)\n",
    "    model[:decode] = [ init(2H,4H), init(1,4H) ]\n",
    "    model[:output] = [ init(H,V), init(1,V) ]\n",
    "    return model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s2s (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2S loss function\n",
    "\n",
    "function s2s(model, inputs, outputs)\n",
    "    state = initstate(inputs[1], model[:state0])\n",
    "    for input in inputs\n",
    "        input = onehotrows(input, model[:embed1])\n",
    "        input = input * model[:embed1]\n",
    "        state = lstm(model[:encode], state, input)\n",
    "    end\n",
    "    EOS = eosmatrix(outputs[1], model[:embed2])\n",
    "    input = EOS * model[:embed2]\n",
    "    sumlogp = 0\n",
    "    for output in outputs\n",
    "        state = lstm(model[:decode], state, input)\n",
    "        ypred = predict(model[:output], state[1])\n",
    "        ygold = onehotrows(output, model[:embed2])\n",
    "        sumlogp += sum(ygold .* logp(ypred,2))\n",
    "        input = ygold * model[:embed2]\n",
    "    end\n",
    "    state = lstm(model[:decode], state, input)\n",
    "    ypred = predict(model[:output], state[1])\n",
    "    sumlogp += sum(EOS .* logp(ypred,2))\n",
    "    return -sumlogp\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2S helper functions\n",
    "\n",
    "function predict(param, input)\n",
    "    input * param[1] .+ param[2]\n",
    "end\n",
    "\n",
    "function initstate(idx, state0)\n",
    "    h,c = state0\n",
    "    h = h .+ fill!(similar(AutoGrad.getval(h), length(idx), length(h)), 0)\n",
    "    c = c .+ fill!(similar(AutoGrad.getval(c), length(idx), length(c)), 0)\n",
    "    return (h,c)\n",
    "end\n",
    "\n",
    "function onehotrows(idx, embeddings)\n",
    "    nrows,ncols = length(idx), size(embeddings,1)\n",
    "    z = zeros(Float32,nrows,ncols)\n",
    "    @inbounds for i=1:nrows\n",
    "        z[i,idx[i]] = 1\n",
    "    end\n",
    "    oftype(AutoGrad.getval(embeddings),z)\n",
    "end\n",
    "\n",
    "let EOS=nothing; global eosmatrix\n",
    "function eosmatrix(idx, embeddings)\n",
    "    nrows,ncols = length(idx), size(embeddings,1)\n",
    "    if EOS==nothing || size(EOS) != (nrows,ncols)\n",
    "        EOS = zeros(Float32,nrows,ncols)\n",
    "        EOS[:,1] = 1\n",
    "        EOS = oftype(AutoGrad.getval(embeddings), EOS)\n",
    "    end\n",
    "    return EOS\n",
    "end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reversing English words as an example task\n",
    "# This loads them from /usr/share/dict/words and converts each character to an int.\n",
    "\n",
    "function readdata(file=\"/usr/share/dict/words\")\n",
    "    global strings = map(chomp,readlines(file))\n",
    "    global tok2int = Dict{Char,Int}()\n",
    "    global int2tok = Vector{Char}()\n",
    "    push!(int2tok,'\\n'); tok2int['\\n']=1 # We use '\\n'=>1 as the EOS token                                                 \n",
    "    sequences = Vector{Vector{Int}}()\n",
    "    for w in strings\n",
    "        s = Vector{Int}()\n",
    "        for c in collect(w)\n",
    "            if !haskey(tok2int,c)\n",
    "                push!(int2tok,c)\n",
    "                tok2int[c] = length(int2tok)\n",
    "            end\n",
    "            push!(s, tok2int[c])\n",
    "        end\n",
    "        push!(sequences, s)\n",
    "    end\n",
    "    return sequences\n",
    "end\n",
    "\n",
    "sequences = readdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch sequences putting equal length sequences together:\n",
    "\n",
    "function minibatch(sequences, batchsize)\n",
    "    table = Dict{Int,Vector{Vector{Int}}}()\n",
    "    data = Any[]\n",
    "    for s in sequences\n",
    "        n = length(s)\n",
    "        nsequences = get!(table, n, Any[])\n",
    "        push!(nsequences, s)\n",
    "        if length(nsequences) == batchsize\n",
    "            push!(data, [[ nsequences[i][j] for i in 1:batchsize] for j in 1:n ])\n",
    "            empty!(nsequences)\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 128, 70)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize, statesize, vocabsize = 100, 128, length(int2tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = minibatch(sequences,batchsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nothing; knetgc() # clean memory from previous run\n",
    "model = initmodel(statesize,vocabsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "function avgloss(model, data)\n",
    "    sumloss = cntloss = 0\n",
    "    for sequence in data\n",
    "        tokens = (1 + length(sequence)) * length(sequence[1])\n",
    "        sumloss += s2s(model, sequence, reverse(sequence))\n",
    "        cntloss += tokens\n",
    "    end\n",
    "    return sumloss/cntloss\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.263982f0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgloss(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.0925f0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2sgrad = grad(s2s)\n",
    "\n",
    "function train(model, data, opts)\n",
    "    for sequence in data\n",
    "        grads = s2sgrad(model, sequence, reverse(sequence))\n",
    "        update!(model, grads, opts)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = optimizers(model,Adam);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time train(model,data,opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30980343f0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgloss(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69.554472 seconds (103.27 M allocations: 6.256 GiB, 2.06% gc time)\n",
      "(1, 0.15178539f0)\n",
      " 69.657559 seconds (103.28 M allocations: 6.256 GiB, 2.03% gc time)\n",
      "(2, 0.10038032f0)\n",
      " 69.321889 seconds (103.28 M allocations: 6.256 GiB, 2.03% gc time)\n",
      "(3, 0.095697075f0)\n",
      " 69.407281 seconds (103.28 M allocations: 6.256 GiB, 2.03% gc time)\n",
      "(4, 0.03121266f0)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1msum_outgrads\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Dict{Symbol,Any}, ::AutoGrad.UngetIndex\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/dev/shm/dyuret/.julia/v0.6/AutoGrad/src/interfaces.jl:89\u001b[22m\u001b[22m",
      " [2] \u001b[1mbackward_pass\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Dict{Symbol,Any}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/dev/shm/dyuret/.julia/v0.6/AutoGrad/src/core.jl:258\u001b[22m\u001b[22m",
      " [3] \u001b[1m(::AutoGrad.##gradfun#1#3{#s2s,Int64})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Dict{Symbol,Any}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/dev/shm/dyuret/.julia/v0.6/AutoGrad/src/core.jl:40\u001b[22m\u001b[22m",
      " [4] \u001b[1m(::AutoGrad.#gradfun#2)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Dict{Symbol,Any}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/dev/shm/dyuret/.julia/v0.6/AutoGrad/src/core.jl:39\u001b[22m\u001b[22m",
      " [5] \u001b[1mtrain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Dict{Symbol,Any}, ::Array{Any,1}, ::Dict{Symbol,Any}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[44]:5\u001b[22m\u001b[22m",
      " [6] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [7] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[48]:2\u001b[22m\u001b[22m [inlined]",
      " [8] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [9] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [10] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "@time for epoch=1:10\n",
    "    @time train(model,data,opts)\n",
    "    println((epoch,avgloss(model,data)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on some examples:\n",
    "\n",
    "function translate(model, str)\n",
    "    state = model[:state0]\n",
    "    for c in reverse(collect(str))\n",
    "        input = onehotrows(tok2int[c], model[:embed1])\n",
    "        input = input * model[:embed1]\n",
    "        state = lstm(model[:encode], state, input)\n",
    "    end\n",
    "    input = eosmatrix(1, model[:embed2]) * model[:embed2]\n",
    "    output = Char[]\n",
    "    for i=1:100 #while true                                                                                                \n",
    "        state = lstm(model[:decode], state, input)\n",
    "        pred = predict(model[:output], state[1])\n",
    "        i = indmax(Array(pred))\n",
    "        i == 1 && break\n",
    "        push!(output, int2tok[i])\n",
    "        input = onehotrows(i, model[:embed2]) * model[:embed2]\n",
    "    end\n",
    "    String(output)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"capricorn\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model,\"capricorn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
